{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-05 23:24:21.099831: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-09-05 23:24:21.099860: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow.examples'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_9174/3749033404.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;31m# baixa os dados na pasta criada e carrega os dados\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexamples\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtutorials\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmnist\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0minput_data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_data_sets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"tmp/\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mone_hot\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow.examples'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import numpy as np # para computação numética menos intensiva\n",
    "import os # para criar pastas\n",
    "from matplotlib import pyplot as plt # para mostrar imagens\n",
    "import tensorflow as tf # para redes neurais\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "# criamos uma pasta para salvar o modelo\n",
    "if not os.path.exists('tmp'): # se a pasta não existir\n",
    "\tos.makedirs('tmp') # cria a pasta para guardar os dados\n",
    "\n",
    "# baixa os dados na pasta criada e carrega os dados\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "data = input_data.read_data_sets(\"tmp/\", one_hot=False)\n",
    "\n",
    "## Funções de Ativação\n",
    "def sig(x, derivative=False):\n",
    "\tif derivative:\n",
    "\t\treturn sig(x)*(1-sig(x))\n",
    "\t\n",
    "\telse:\n",
    "\t\treturn 1.0 / (1+np.exp(-x))\n",
    "\n",
    "def tanh(x, derivative=False):\n",
    "\tif derivative:\n",
    "\t\treturn 1 - tanh(x) ** 2\n",
    "\t\n",
    "\telse:\n",
    "\t\treturn 2*sig(2*x) - 1\n",
    "\n",
    "\n",
    "def ReLU(x, derivative=False):\n",
    "\tif derivative:\n",
    "\t\treturn (np.zeros_like(x) <= x).astype(np.float32)\n",
    "\n",
    "\telse:\n",
    "\t\treturn np.maximum(np.zeros_like(x), x)\n",
    "\n",
    "\n",
    "def Leaky_ReLU(x, alpha = 0.2, derivative=False):\n",
    "\tassert alpha < 1 and alpha > 0\n",
    "\n",
    "\tif derivative:\n",
    "\t\tless_than_zero = (np.zeros_like(x) >= x).astype(np.float32)\n",
    "\t\tless_than_zero *= alpha\n",
    "\t\tgrt_than_zero = (np.zeros_like(x) < x).astype(np.float32)\n",
    "\t\treturn less_than_zero + grt_than_zero\n",
    "\n",
    "\telse:\n",
    "\t\treturn np.maximum(x*alpha, x)\n",
    "\n",
    "\n",
    "def ELU(x, alpha=1, derivative=False):\n",
    "\tif derivative:\n",
    "\t\tless_than_zero = (np.zeros_like(x) >= x).astype(np.float32)\n",
    "\t\tless_than_zero *= ELU(x)+alpha \n",
    "\t\tgrt_than_zero = (np.zeros_like(x) < x).astype(np.float32)\n",
    "\t\treturn less_than_zero + grt_than_zero\n",
    "\n",
    "\telse:\n",
    "\t\tless_than_zero = (np.zeros_like(x) >= x).astype(np.float32)\n",
    "\t\tless_than_zero *= (np.exp(x) - 1)*alpha\n",
    "\t\tgrt_than_zero = (np.zeros_like(x) < x).astype(np.float32)\n",
    "\t\tgrt_than_zero *= x \n",
    "\t\treturn less_than_zero + grt_than_zero\n",
    "\n",
    "\n",
    "def plot_func_deriv(func, name):\n",
    "\ti = np.linspace(-5, 5, 500)\n",
    "\tx = func(i)\n",
    "\tdx = func(i, derivative=True)\n",
    "\tplt.plot(i, x, label=name, lw=3)\n",
    "\tplt.plot(i, dx, label='Derivada')\n",
    "\tplt.legend()\n",
    "\t# plt.ylim([-2,3])\n",
    "\tplt.show()\n",
    "\n",
    "def fully_conected_layer(inputs, n_neurons, activation=tf.nn.sigmoid):\n",
    "\t'''\n",
    "\tAdiciona os nós de uma camada ao grafo TensorFlow e\n",
    "\tretorna o tensor de saída da camada.\n",
    "\tArgs:\n",
    "\t\tinputs: um tensor de entrada da camda\n",
    "\t\tn_neurons: a qtd de neurônios da camada\n",
    "\t\tactivation: a função de ativação da camada (padrão: tf.nn.sigmoid)\n",
    "\t'''\n",
    "\t# define as variáveis da camada\n",
    "\tn_inputs = int(inputs.get_shape()[1]) # pega o formato dos inputs\n",
    "\t# usa uma semente para garantir a consitência na inicialização aleatória\n",
    "\tW = tf.Variable(tf.truncated_normal([n_inputs, n_neurons], seed=1)) \n",
    "\tb = tf.Variable(tf.zeros([n_neurons]), name='biases')\n",
    "\t\n",
    "\t# operação linar da camada\n",
    "\tlayer = tf.add(tf.matmul(inputs, W), b, name='linear_transformation')\n",
    "\t\n",
    "\t# aplica não linearidade, se for o caso\n",
    "\tif activation is None:\n",
    "\t\treturn layer\n",
    "\telse:\n",
    "\t\treturn activation(layer)\n",
    "\t\n",
    "\n",
    "def\tleaky_relu(z, leak=0.01):\n",
    "\t'''Cria uma função de ativação leaky ReLU'''\n",
    "\treturn tf.maximum(leak * z, z)\n",
    "\n",
    "\n",
    "def net(X_tensor, y_tensor, activation=tf.nn.sigmoid):\n",
    "\t'''\n",
    "\tAdiciona ao grafo os nós de uma rede neural.\n",
    "\tRetorna um tuple (opt, acc), com o nó de otimização da rede neural e o nó de acurácia\n",
    "\t'''\n",
    "\n",
    "\t# Monta uma rede neural simples, com duas camadas e 512 neurônios por camada\n",
    "\tl1 = fully_conected_layer(X_tensor, n_neurons=512, activation=activation)\n",
    "\tl2 = fully_conected_layer(l1, n_neurons=512, activation=activation)\n",
    "\tlogit = fully_conected_layer(l2, n_neurons=10, activation=None)\n",
    "\n",
    "\t# computa o erro e faz o nó de otimização\n",
    "\terror = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y_input, logits=logit))\n",
    "\ttrain_step = tf.train.GradientDescentOptimizer(learning_rate=0.01).minimize(error)\n",
    "\n",
    "\t# calcula acurácia\n",
    "\tcorrect = tf.nn.in_top_k(logit, y_tensor, 1) # calcula obs corretas\n",
    "\taccuracy = tf.reduce_mean(tf.cast(correct, tf.float32)) # converte para float32\n",
    "\n",
    "\treturn (train_step, accuracy)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\t\n",
    "\tgraph = tf.Graph()\n",
    "\twith graph.as_default():\n",
    "\n",
    "\t\tx_input = tf.placeholder(tf.float32, [None, 28*28])\n",
    "\t\ty_input = tf.placeholder(tf.int64, [None])\n",
    "\n",
    "\t\t# cria uma rede neural para cada função de ativação\n",
    "\t\tsig_step, sig_acc = net(x_input, y_input, activation=tf.nn.sigmoid)\n",
    "\t\ttanh_step, tanh_acc = net(x_input, y_input, activation=tf.nn.tanh)\n",
    "\t\trelu_step, relu_acc = net(x_input, y_input, activation=tf.nn.relu)\n",
    "\t\telu_step, elu_acc = net(x_input, y_input, activation=tf.nn.elu)\n",
    "\t\tleaky_relu_step, leaky_relu_acc = net(x_input, y_input, activation=leaky_relu)\n",
    "\n",
    "\t\t# junta todos os passos te otimização e acurácias. Vamos iterar por eles depois.\n",
    "\t\topt_steps = [sig_step, tanh_step, relu_step, elu_step, leaky_relu_step]\n",
    "\t\tacuracies = [sig_acc, tanh_acc, relu_acc, elu_acc, leaky_relu_acc]\n",
    "\n",
    "\t\tinit = tf.global_variables_initializer()\n",
    "\n",
    "\n",
    "\twith tf.Session(graph=graph) as sess:\n",
    "\t\tinit.run() # iniciamos as variáveis\n",
    "\n",
    "\t\t# pega 1000 amostras do set de teste para avaialção\n",
    "\t\ttest_x, text_y = data.test.next_batch(1000)\n",
    "\t\ttest_dict = {x_input: test_x, y_input: text_y}\n",
    "\n",
    "\t\t# loop de treinamento\n",
    "\t\tall_accs = []\n",
    "\t\tfor step in range(1001):\n",
    "\n",
    "\t\t\t# monta os mini-lotes\n",
    "\t\t\tx_batch, y_batch = data.train.next_batch(64)\n",
    "\t\t\tfeed_dict = {x_input: x_batch, y_input: y_batch}\n",
    "\n",
    "\t\t\t# uma iteração de treino para cada rede\n",
    "\t\t\tfor opt in opt_steps:\n",
    "\t\t\t\tsess.run(opt, feed_dict=feed_dict) # roda uma iteração de treino\n",
    "\t\t\t\n",
    "\t\t\t# a cada 10 passos, calcula a acurácia no set de teste.\n",
    "\t\t\tif step % 10 == 0:\n",
    "\t\t\t\tacc_list = []\n",
    "\t\t\t\tfor acc in acuracies:\n",
    "\t\t\t\t\ta = sess.run(acc, feed_dict=test_dict)\n",
    "\t\t\t\t\tacc_list.append(a)\n",
    "\n",
    "\t\t\t\tall_accs.append(acc_list)\n",
    "\n",
    "\tdf = pd.DataFrame(np.array(all_accs) * 100)\n",
    "\tdf.plot()\n",
    "\tplt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
